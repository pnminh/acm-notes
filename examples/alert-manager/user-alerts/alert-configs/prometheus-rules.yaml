apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-alertmanager-main-rules
spec:
  groups:
    - name: alertmanager.rules
      rules:
        - alert: AlertmanagerFailedReload
          annotations:
            description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md
            summary: Reloading an Alertmanager configuration has failed.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(alertmanager_config_last_reload_successful{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) == 0
          for: 10m
          labels:
            severity: critical
        - alert: AlertmanagerMembersInconsistent
          annotations:
            description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
            summary: A member of an Alertmanager cluster has not found all other cluster members.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
              max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
            < on (namespace,service) group_left
              count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m]))
          for: 15m
          labels:
            severity: warning
        - alert: AlertmanagerFailedToSendAlerts
          annotations:
            description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md
            summary: An Alertmanager instance failed to send notifications.
          expr: |
            (
              rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
            /
              rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
            )
            > 0.01
          for: 5m
          labels:
            severity: warning
        - alert: AlertmanagerClusterFailedToSendAlerts
          annotations:
            description: The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md
            summary: All Alertmanager instances in a cluster failed to send notifications to a critical integration.
          expr: |
            min by (namespace,service, integration) (
              rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
            /
              rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
            )
            > 0.01
          for: 5m
          labels:
            severity: warning
        - alert: AlertmanagerConfigInconsistent
          annotations:
            description: Alertmanager instances within the {{$labels.job}} cluster have different configurations.
            summary: Alertmanager instances within the same cluster have different configurations.
          expr: |
            count by (namespace,service) (
              count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job=~"alertmanager-main|alertmanager-user-workload"})
            )
            != 1
          for: 20m
          labels:
            severity: warning
        - alert: AlertmanagerClusterDown
          annotations:
            description: '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.'
            summary: Half or more of the Alertmanager instances within the same cluster are down.
          expr: |
            (
              count by (namespace,service) (
                avg_over_time(up{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) < 0.5
              )
            /
              count by (namespace,service) (
                up{job=~"alertmanager-main|alertmanager-user-workload"}
              )
            )
            >= 0.5
          for: 5m
          labels:
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-aws-efs-alerts
spec:
  groups:
    - name: aws-efs-alerts
      rules:
        - alert: MultipleVersionsOfEFSCSIDriverInstalled
          annotations:
            message: Multiple versions of EFS CSI Driver installed
          expr: count(kube_pod_info{namespace="openshift-operators", pod=~"efs-csi-node-.*"}) > 0 and count(kube_pod_info{namespace="openshift-cluster-csi-drivers", pod=~"aws-efs-csi-driver-operator-.*"}) > 0
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/MultipleVersionsOfEFSCSIDriverInstalled.md
            managed_notification_template: MultipleEFSCSIDrivers
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: critical
            source: https://issues.redhat.com/browse/OSD-16223
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-cluster-monitoring-operator-prometheus-rules
spec:
  groups:
    - name: openshift-general.rules
      rules:
        - alert: TargetDown
          annotations:
            description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.'
            summary: Some targets were not reachable from the monitoring server for an extended period of time.
          expr: |
            100 * ((
              1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /
                  count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)
            ) or (
              count by (job, namespace, service) (up == 0) /
              count by (job, namespace, service) (up)
            )) > 10
          for: 15m
          labels:
            severity: warning
    - name: openshift-kubernetes.rules
      rules:
        - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) BY (pod, namespace)
          record: pod:container_cpu_usage:sum
        - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
          record: pod:container_fs_usage_bytes:sum
        - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
          record: namespace:container_memory_usage_bytes:sum
        - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m])) BY (namespace)
          record: namespace:container_cpu_usage:sum
        - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)
          record: cluster:memory_usage:ratio
        - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
          record: cluster:container_spec_cpu_shares:ratio
        - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) / sum(machine_cpu_cores)
          record: cluster:container_cpu_usage:ratio
        - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role="master"})
          labels:
            label_node_role_kubernetes_io: master
            label_node_role_kubernetes_io_master: "true"
          record: cluster:master_nodes
        - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role="infra"})
          labels:
            label_node_role_kubernetes_io_infra: "true"
          record: cluster:infra_nodes
        - expr: max without(endpoint, instance, job, pod, service) (cluster:master_nodes and on(node) cluster:infra_nodes)
          labels:
            label_node_role_kubernetes_io_infra: "true"
            label_node_role_kubernetes_io_master: "true"
          record: cluster:master_infra_nodes
        - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node) cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod, service) (kube_node_labels)
          record: cluster:nodes_roles
        - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node", "$1", "instance", "(.*)")) by (node, package, core) == 2)
          labels:
            label_node_hyperthread_enabled: "true"
          record: cluster:hyperthread_enabled_nodes
        - expr: count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name)
          record: cluster:virt_platform_nodes:sum
        - expr: |
            sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (
              (
                cluster:master_nodes
                * on(node) group_left() max by(node)
                (
                  kube_node_status_capacity{resource="cpu",unit="core"}
                )
              )
              or on(node) (
                label_replace(cluster:infra_nodes, "label_node_role_kubernetes_io", "infra", "", "")
                * on(node) group_left() max by(node)
                (
                  kube_node_status_capacity{resource="cpu",unit="core"}
                )
              )
              or on(node) (
                max without(endpoint, instance, job, pod, service)
                (
                  kube_node_labels
                ) * on(node) group_left() max by(node)
                (
                  kube_node_status_capacity{resource="cpu",unit="core"}
                )
              )
            )
          record: cluster:capacity_cpu_cores:sum
        - expr: |
            clamp_max(
              label_replace(
                sum by(instance, package, core) (
                  node_cpu_info{core!="",package!=""}
                  or
                  # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                  label_replace(label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
                ) > 1,
                "label_node_hyperthread_enabled",
                "true",
                "instance",
                "(.*)"
              ) or on (instance, package)
              label_replace(
                sum by(instance, package, core) (
                  label_replace(node_cpu_info{core!="",package!=""}
                  or
                  # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                  label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
                ) <= 1,
                "label_node_hyperthread_enabled",
                "false",
                "instance",
                "(.*)"
              ),
              1
            )
          record: cluster:cpu_core_hyperthreading
        - expr: |
            topk by(node) (1, cluster:nodes_roles) * on (node)
              group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                           label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
            label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
          record: cluster:cpu_core_node_labels
        - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled)
          record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
        - expr: |
            sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
            (
              (
                cluster:master_nodes
                * on(node) group_left() max by(node)
                (
                  kube_node_status_capacity{resource="memory",unit="byte"}
                )
              )
              or on(node)
              (
                max without(endpoint, instance, job, pod, service)
                (
                  kube_node_labels
                )
                * on(node) group_left() max by(node)
                (
                  kube_node_status_capacity{resource="memory",unit="byte"}
                )
              )
            )
          record: cluster:capacity_memory_bytes:sum
        - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~"node-exporter.+"})
          record: cluster:cpu_usage_cores:sum
        - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
          record: cluster:memory_usage_bytes:sum
        - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
          record: workload:cpu_usage_cores:sum
        - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
          record: openshift:cpu_usage_cores:sum
        - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
          record: workload:memory_usage_bytes:sum
        - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
          record: openshift:memory_usage_bytes:sum
        - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)
          record: cluster:node_instance_type_count:sum
        - expr: |
            sum by(provisioner) (
              topk by (namespace, persistentvolumeclaim) (
                1, kube_persistentvolumeclaim_resource_requests_storage_bytes
              ) * on(namespace, persistentvolumeclaim) group_right()
              topk by(namespace, persistentvolumeclaim) (
                1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
              )
            )
          record: cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum
        - expr: (sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="",label_node_role_kubernetes_io_infra=""} or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="true"} or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1) or (absent(cluster_master_schedulable == 1)*0))))
          record: workload:capacity_physical_cpu_cores:sum
        - expr: min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
          record: cluster:usage:workload:capacity_physical_cpu_cores:min:5m
        - expr: max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
          record: cluster:usage:workload:capacity_physical_cpu_cores:max:5m
        - expr: |
            sum  by (provisioner) (
              topk by (namespace, persistentvolumeclaim) (
                1, kubelet_volume_stats_used_bytes
              ) * on (namespace,persistentvolumeclaim) group_right()
              topk by (namespace, persistentvolumeclaim) (
                1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
              )
            )
          record: cluster:kubelet_volume_stats_used_bytes:provisioner:sum
        - expr: sum by (instance) (apiserver_storage_objects)
          record: instance:etcd_object_counts:sum
        - expr: topk(500, max by(resource) (apiserver_storage_objects))
          record: cluster:usage:resources:sum
        - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"}) by (namespace,pod))
          record: cluster:usage:pods:terminal:workload:sum
        - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
          record: cluster:usage:containers:sum
        - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
          record: node_role_os_version_machine:cpu_capacity_cores:sum
        - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io)
          record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
        - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra) ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
          record: node_role_os_version_machine:cpu_capacity_sockets:sum
        - expr: max(alertmanager_integrations{namespace="openshift-monitoring"})
          record: cluster:alertmanager_integrations:max
        - expr: sum by(plugin_name, volume_mode)(pv_collector_total_pv_count)
          record: cluster:kube_persistentvolume_plugin_type_counts:sum
        - expr: |
            sum(
              min by (node) (kube_node_status_condition{condition="Ready",status="true"})
                and
              max by (node) (kube_node_role{role="master"})
            ) == bool sum(kube_node_role{role="master"})
          record: cluster:control_plane:all_nodes_ready
        - expr: max by (profile) (cluster_monitoring_operator_collection_profile == 1)
          record: profile:cluster_monitoring_operator_collection_profile:max
        - alert: ClusterMonitoringOperatorReconciliationErrors
          annotations:
            description: Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.
            summary: Cluster Monitoring Operator is experiencing unexpected reconciliation errors.
          expr: max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m]) == 0
          for: 1h
          labels:
            severity: warning
        - alert: AlertmanagerReceiversNotConfigured
          annotations:
            description: Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.
            summary: Receivers (notification integrations) are not configured on Alertmanager
          expr: cluster:alertmanager_integrations:max == 0
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md
            summary: Deployment has not matched the expected number of replicas
          expr: |
            (((
              kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                >
              kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) and (
              changes(kube_deployment_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
                ==
              0
            )) * on() group_left cluster:control_plane:all_nodes_ready) > 0
          for: 15m
          labels:
            severity: warning
        - expr: avg_over_time((((count((max by (node) (up{job="kubelet",metrics_path="/metrics"} == 1) and max by (node) (kube_node_status_condition{condition="Ready",status="true"} == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])
          record: cluster:usage:kube_schedulable_node_ready_reachable:avg5m
        - expr: avg_over_time((count(max by (node) (kube_node_status_condition{condition="Ready",status="true"} == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}))))[5m:1s])
          record: cluster:usage:kube_node_ready:avg5m
        - expr: (max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition="false"} == 1)*0 or (kube_pod_status_ready{condition="true"} == 1)) * on(pod,namespace) group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~"Running|Unknown|Pending"} == 1)))
          record: kube_running_pod_ready
        - expr: avg(kube_running_pod_ready{namespace=~"openshift-.*"})
          record: cluster:usage:openshift:kube_running_pod_ready:avg
        - expr: avg(kube_running_pod_ready{namespace!~"openshift-.*"})
          record: cluster:usage:workload:kube_running_pod_ready:avg
        - alert: KubePodNotScheduled
          annotations:
            description: |-
              Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.
              Check the details of the pod with the following command:
              oc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}
            summary: Pod cannot be scheduled.
          expr: last_over_time(kube_pod_status_unschedulable{namespace=~"(openshift-.*|kube-.*|default)"}[5m]) == 1
          for: 30m
          labels:
            severity: warning
    - interval: 30s
      name: kubernetes-recurring.rules
      rules:
        - expr: sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s)*0))
          record: cluster:usage:workload:capacity_physical_cpu_core_seconds
    - name: openshift-ingress.rules
      rules:
        - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
          record: code:cluster:ingress_http_request_count:rate5m:sum
        - expr: sum (rate(haproxy_frontend_bytes_in_total[5m]))
          record: cluster:usage:ingress_frontend_bytes_in:rate5m:sum
        - expr: sum (rate(haproxy_frontend_bytes_out_total[5m]))
          record: cluster:usage:ingress_frontend_bytes_out:rate5m:sum
        - expr: sum (haproxy_frontend_current_sessions)
          record: cluster:usage:ingress_frontend_connections:sum
        - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace!~"openshift-.*"}[5m]) > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m]))) or absent(__does_not_exist__)*0
          record: cluster:usage:workload:ingress_request_error:fraction5m
        - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m]))) or absent(__does_not_exist__)*0
          record: cluster:usage:workload:ingress_request_total:irate5m
        - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace=~"openshift-.*"}[5m]) > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m]))) or absent(__does_not_exist__)*0
          record: cluster:usage:openshift:ingress_request_error:fraction5m
        - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m]))) or absent(__does_not_exist__)*0
          record: cluster:usage:openshift:ingress_request_total:irate5m
        - expr: sum(ingress_controller_aws_nlb_active) or vector(0)
          record: cluster:ingress_controller_aws_nlb_active:sum
    - name: openshift-build.rules
      rules:
        - expr: sum by (strategy) (openshift_build_status_phase_total)
          record: openshift:build_by_strategy:sum
    - name: openshift-monitoring.rules
      rules:
        - expr: sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}))
          record: openshift:prometheus_tsdb_head_series:sum
        - expr: sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[2m])))
          record: openshift:prometheus_tsdb_head_samples_appended_total:sum
        - expr: sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~"openshift-monitoring|openshift-user-workload-monitoring", container=""}))
          record: monitoring:container_memory_working_set_bytes:sum
        - expr: topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))
          record: namespace_job:scrape_series_added:topk3_sum1h
        - expr: topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))
          record: namespace_job:scrape_samples_post_metric_relabeling:topk3
        - expr: sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace="openshift-monitoring", exported_service=~"alertmanager-main|prometheus-k8s"}[5m]))
          record: monitoring:haproxy_server_http_responses_total:sum
        - expr: max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicationController"},"replicationcontroller", "$1", "owner_name", "(.*)") * on(replicationcontroller, namespace) group_left(owner_name) topk by(replicationcontroller, namespace) (1, max by (replicationcontroller, namespace, owner_name) (kube_replicationcontroller_owner{job="kube-state-metrics"})),"workload", "$1", "owner_name", "(.*)"))
          labels:
            workload_type: deploymentconfig
          record: namespace_workload_pod:kube_pod_owner:relabel
    - name: openshift-etcd-telemetry.rules
      rules:
        - expr: sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job="etcd"})
          record: instance:etcd_mvcc_db_total_size_in_bytes:sum
        - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
          labels:
            quantile: "0.99"
          record: instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile
        - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])))
          labels:
            quantile: "0.99"
          record: instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile
        - expr: sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job="etcd"})
          record: instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum
        - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])))
          labels:
            quantile: "0.99"
          record: instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile
    - name: openshift-sre.rules
      rules:
        - expr: sum(rate(apiserver_request_total{job="apiserver"}[10m])) BY (code)
          record: code:apiserver_request_total:rate:sum
    - name: openshift-vsphere.rules
      rules:
        - expr: sum by(version, build)(vsphere_vcenter_info)
          record: cluster:vsphere_vcenter_info:sum
        - expr: sum by(version)(vsphere_esxi_version_total)
          record: cluster:vsphere_esxi_version_total:sum
        - expr: sum by(hw_version)(vsphere_node_hw_version_total)
          record: cluster:vsphere_node_hw_version_total:sum
        - expr: max by(source)(vsphere_topology_tags)
          record: cluster:vsphere_topology_tags:max
        - expr: max by(scope)(vsphere_infrastructure_failure_domains)
          record: cluster:vsphere_infrastructure_failure_domains:max
        - expr: max by(status)(vsphere_csi_migration{status=~"|LegacyDeprecatedInTreeDriver|CSIWithMigrationDriver"})
          record: cluster:vsphere_csi_migration:max
    - name: apiserver-list-watch.rules
      rules:
        - expr: sum by(verb) (rate(apiserver_request_total{verb=~"LIST|WATCH",code=~"2.."}[5m]))
          record: apiserver_list_watch_request_success_total:rate:sum
    - name: general.rules
      rules:
        - alert: Watchdog
          annotations:
            description: |
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
            summary: An alert that should always be firing to certify that Alertmanager is working properly.
          expr: vector(1)
          labels:
            namespace: openshift-monitoring
            severity: none
    - name: node-network
      rules:
        - alert: NodeNetworkInterfaceFlapping
          annotations:
            description: Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
            summary: Network interface is often changing its status
          expr: |
            changes(node_network_up{job="node-exporter",device!~"veth.+|tunbr"}[2m]) > 2
          for: 2m
          labels:
            severity: warning
    - name: kube-prometheus-node-recording.rules
      rules:
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m])) BY (instance)
          record: instance:node_cpu:rate:sum
        - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
          record: instance:node_network_receive_bytes:rate:sum
        - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
          record: instance:node_network_transmit_bytes:rate:sum
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
          record: cluster:node_cpu:sum_rate5m
        - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))
          record: cluster:node_cpu:ratio
    - name: kube-prometheus-general.rules
      rules:
        - expr: count without(instance, pod, node) (up == 1)
          record: count:up1
        - expr: count without(instance, pod, node) (up == 0)
          record: count:up0
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-kube-state-metrics-rules
spec:
  groups:
    - name: kube-state-metrics
      rules:
        - alert: KubeStateMetricsListErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            summary: kube-state-metrics is experiencing errors in list operations.
          expr: |
            (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
              /
            sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
            > 0.01
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: KubeStateMetricsWatchErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            summary: kube-state-metrics is experiencing errors in watch operations.
          expr: |
            (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
              /
            sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
            > 0.01
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-kubernetes-monitoring-rules
spec:
  groups:
    - name: kubernetes-apps
      rules:
        - alert: KubePodCrashLooping
          annotations:
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
            summary: Pod is crash looping.
          expr: |
            max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m]) >= 1
          for: 15m
          labels:
            severity: warning
        - alert: KubePodNotReady
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md
            summary: Pod has been in a non-ready state for more than 15 minutes.
          expr: |
            sum by (namespace, pod, cluster) (
              max by(namespace, pod, cluster) (
                kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default)", job="kube-state-metrics", phase=~"Pending|Unknown"}
                unless ignoring(phase) (kube_pod_status_unschedulable{job="kube-state-metrics"} == 1)
              ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
                1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
              )
            ) > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeDeploymentGenerationMismatch
          annotations:
            description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            summary: Deployment generation mismatch due to possible roll-back
          expr: |
            kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
        - alert: KubeDeploymentRolloutStuck
          annotations:
            description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.
            summary: Deployment rollout is not progressing.
          expr: |
            kube_deployment_status_condition{condition="Progressing", status="false",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            != 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
            summary: StatefulSet has not matched the expected number of replicas.
          expr: |
            (
              kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) and (
              changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[10m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
            summary: StatefulSet generation mismatch due to possible roll-back
          expr: |
            kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
            summary: StatefulSet update has not been rolled out.
          expr: |
            (
              max without (revision) (
                kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              )
            )  and (
              changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 30 minutes.
            summary: DaemonSet rollout is stuck.
          expr: |
            (
              (
                kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                 !=
                0
              ) or (
                kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              )
            ) and (
              changes(kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 30m
          labels:
            severity: warning
        - alert: KubeContainerWaiting
          annotations:
            description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
            summary: Pod container waiting longer than 1 hour
          expr: |
            sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}) > 0
          for: 1h
          labels:
            severity: warning
        - alert: KubeDaemonSetNotScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
            summary: DaemonSet pods are not scheduled.
          expr: |
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              -
            kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubeDaemonSetMisScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
            summary: DaemonSet pods are misscheduled.
          expr: |
            kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeJobNotCompleted
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
            summary: Job did not complete in time
          expr: |
            time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              and
            kube_job_status_active{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0) > 43200
          labels:
            severity: warning
        - alert: KubeJobFailed
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md
            summary: Job failed to complete.
          expr: |
            kube_job_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}  > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeHpaReplicasMismatch
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
            summary: HPA has not matched desired number of replicas.
          expr: |
            (kube_horizontalpodautoscaler_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              >
            kube_horizontalpodautoscaler_spec_min_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              <
            kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
              and
            changes(kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[15m]) == 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeHpaMaxedOut
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
            summary: HPA is running at max replicas
          expr: |
            kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              ==
            kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
    - name: kubernetes-resources
      rules:
        - alert: KubeCPUOvercommit
          annotations:
            description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
            summary: Cluster has overcommitted CPU resource requests.
          expr: |
            sum(namespace_cpu:kube_pod_container_resource_requests:sum{job="kube-state-metrics",}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
            and
            (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
          for: 10m
          labels:
            namespace: kube-system
            severity: warning
        - alert: KubeMemoryOvercommit
          annotations:
            description: Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
            summary: Cluster has overcommitted memory resource requests.
          expr: |
            sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
            and
            (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
          for: 10m
          labels:
            namespace: kube-system
            severity: warning
        - alert: KubeQuotaAlmostFull
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota is going to be full.
          expr: |
            kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
              > 0.9 < 1
          for: 15m
          labels:
            severity: info
        - alert: KubeQuotaFullyUsed
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota is fully used.
          expr: |
            kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
              == 1
          for: 15m
          labels:
            severity: info
        - alert: KubeQuotaExceeded
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
            summary: Namespace quota has exceeded the limits.
          expr: |
            kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
              > 1
          for: 15m
          labels:
            severity: warning
    - name: kubernetes-storage
      rules:
        - alert: KubePersistentVolumeFillingUp
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
            summary: PersistentVolume is filling up.
          expr: |
            (
              kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
                /
              kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            ) < 0.03
            and
            kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
          for: 1m
          labels:
            severity: critical
        - alert: KubePersistentVolumeFillingUp
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
            summary: PersistentVolume is filling up.
          expr: |
            (
              kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
                /
              kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            ) < 0.15
            and
            kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
            and
            predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
          for: 1h
          labels:
            severity: warning
        - alert: KubePersistentVolumeInodesFillingUp
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
            summary: PersistentVolumeInodes are filling up.
          expr: |
            (
              kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
                /
              kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            ) < 0.03
            and
            kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
          for: 1m
          labels:
            severity: critical
        - alert: KubePersistentVolumeInodesFillingUp
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
            summary: PersistentVolumeInodes are filling up.
          expr: |
            (
              kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
                /
              kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            ) < 0.15
            and
            kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
            and
            predict_linear(kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
          for: 1h
          labels:
            severity: warning
        - alert: KubePersistentVolumeErrors
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
            summary: PersistentVolume is having issues with provisioning.
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
          for: 5m
          labels:
            severity: warning
    - name: kubernetes-system
      rules:
        - alert: KubeClientErrors
          annotations:
            description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
            summary: Kubernetes API server client is experiencing errors.
          expr: |
            (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
              /
            sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
            > 0.01
          for: 15m
          labels:
            severity: warning
    - name: kubernetes-system-apiserver
      rules:
        - alert: KubeAggregatedAPIErrors
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.
            summary: Kubernetes aggregated API has reported errors.
          expr: |
            sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total{job="apiserver"}[10m])) > 4
          labels:
            severity: warning
        - alert: KubeAggregatedAPIDown
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.
            summary: Kubernetes aggregated API is down.
          expr: |
            (1 - max by(name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice{job="apiserver"}[10m]))) * 100 < 85
          for: 15m
          labels:
            severity: warning
        - alert: KubeAPIDown
          annotations:
            description: KubeAPI has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeAPIDown.md
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="apiserver"} == 1)
          for: 15m
          labels:
            severity: critical
        - alert: KubeAPITerminatedRequests
          annotations:
            description: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
            summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
          expr: |
            sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m]))  / (  sum(rate(apiserver_request_total{job="apiserver"}[10m])) + sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m])) ) > 0.20
          for: 5m
          labels:
            severity: warning
    - name: kubernetes-system-kubelet
      rules:
        - alert: KubeNodeNotReady
          annotations:
            description: '{{ $labels.node }} has been unready for more than 15 minutes.'
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md
            summary: Node is not ready.
          expr: |
            kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeNodeUnreachable
          annotations:
            description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
            summary: Node is unreachable.
          expr: |
            (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
          for: 15m
          labels:
            severity: warning
        - alert: KubeletTooManyPods
          annotations:
            description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
            summary: Kubelet is running at capacity.
          expr: |
            count by(cluster, node) (
              (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
            )
            /
            max by(cluster, node) (
              kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
            ) > 0.95
          for: 15m
          labels:
            namespace: kube-system
            severity: info
        - alert: KubeNodeReadinessFlapping
          annotations:
            description: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
            summary: Node readiness status is flapping.
          expr: |
            sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
          for: 15m
          labels:
            namespace: kube-system
            severity: warning
        - alert: KubeletPlegDurationHigh
          annotations:
            description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
            summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
          expr: |
            node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
          for: 5m
          labels:
            namespace: kube-system
            severity: warning
        - alert: KubeletPodStartUpLatencyHigh
          annotations:
            description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
            summary: Kubelet Pod startup latency is too high.
          expr: |
            histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
          for: 15m
          labels:
            namespace: kube-system
            severity: warning
        - alert: KubeletClientCertificateRenewalErrors
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
            summary: Kubelet has failed to renew its client certificate.
          expr: |
            increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeletServerCertificateRenewalErrors
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
            summary: Kubelet has failed to renew its server certificate.
          expr: |
            increase(kubelet_server_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeletDown
          annotations:
            description: Kubelet has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="kubelet", metrics_path="/metrics"} == 1)
          for: 15m
          labels:
            namespace: kube-system
            severity: critical
    - name: k8s.rules
      rules:
        - expr: |
            sum by (cluster, namespace, pod, container) (
              irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
            ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
              1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
            )
          record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
        - expr: |
            container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
              max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
            )
          record: node_namespace_pod_container:container_memory_working_set_bytes
        - expr: |
            container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
              max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
            )
          record: node_namespace_pod_container:container_memory_rss
        - expr: |
            container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
              max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
            )
          record: node_namespace_pod_container:container_memory_cache
        - expr: |
            container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
              max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
            )
          record: node_namespace_pod_container:container_memory_swap
        - expr: |
            kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
            group_left() max by (namespace, pod, cluster) (
              (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
            )
          record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
        - expr: |
            sum by (namespace, cluster) (
                sum by (namespace, pod, cluster) (
                    max by (namespace, pod, container, cluster) (
                      kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                    ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace_memory:kube_pod_container_resource_requests:sum
        - expr: |
            kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
            group_left() max by (namespace, pod, cluster) (
              (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
            )
          record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
        - expr: |
            sum by (namespace, cluster) (
                sum by (namespace, pod, cluster) (
                    max by (namespace, pod, container, cluster) (
                      kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                    ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace_cpu:kube_pod_container_resource_requests:sum
        - expr: |
            kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
            group_left() max by (namespace, pod, cluster) (
              (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
            )
          record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
        - expr: |
            sum by (namespace, cluster) (
                sum by (namespace, pod, cluster) (
                    max by (namespace, pod, container, cluster) (
                      kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                    ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace_memory:kube_pod_container_resource_limits:sum
        - expr: |
            kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
            group_left() max by (namespace, pod, cluster) (
             (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
             )
          record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
        - expr: |
            sum by (namespace, cluster) (
                sum by (namespace, pod, cluster) (
                    max by (namespace, pod, container, cluster) (
                      kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                    ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace_cpu:kube_pod_container_resource_limits:sum
        - expr: |
            max by (cluster, namespace, workload, pod) (
              label_replace(
                label_replace(
                  kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                  "replicaset", "$1", "owner_name", "(.*)"
                ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                  1, max by (replicaset, namespace, owner_name) (
                    kube_replicaset_owner{job="kube-state-metrics"}
                  )
                ),
                "workload", "$1", "owner_name", "(.*)"
              )
            )
          labels:
            workload_type: deployment
          record: namespace_workload_pod:kube_pod_owner:relabel
        - expr: |
            max by (cluster, namespace, workload, pod) (
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            )
          labels:
            workload_type: daemonset
          record: namespace_workload_pod:kube_pod_owner:relabel
        - expr: |
            max by (cluster, namespace, workload, pod) (
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            )
          labels:
            workload_type: statefulset
          record: namespace_workload_pod:kube_pod_owner:relabel
        - expr: |
            max by (cluster, namespace, workload, pod) (
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
                "workload", "$1", "owner_name", "(.*)"
              )
            )
          labels:
            workload_type: job
          record: namespace_workload_pod:kube_pod_owner:relabel
    - name: kube-scheduler.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: node.rules
      rules:
        - expr: |
            topk by(cluster, namespace, pod) (1,
              max by (cluster, node, namespace, pod) (
                label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
            ))
          record: 'node_namespace_pod:kube_pod_info:'
        - expr: |
            sum(
              node_memory_MemAvailable_bytes{job="node-exporter"} or
              (
                node_memory_Buffers_bytes{job="node-exporter"} +
                node_memory_Cached_bytes{job="node-exporter"} +
                node_memory_MemFree_bytes{job="node-exporter"} +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            ) by (cluster)
          record: :node_memory_MemAvailable_bytes:sum
        - expr: |
            avg by (cluster, node) (
              sum without (mode) (
                rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
              )
            )
          record: node:node_cpu_utilization:ratio_rate5m
        - expr: |
            avg by (cluster) (
              node:node_cpu_utilization:ratio_rate5m
            )
          record: cluster:node_cpu:ratio_rate5m
    - name: kubelet.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
          labels:
            quantile: "0.99"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
          labels:
            quantile: "0.9"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
          labels:
            quantile: "0.5"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-node-exporter-rules
spec:
  groups:
    - name: node-exporter
      rules:
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
            summary: Filesystem is predicted to run out of space within the next 24 hours.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 15
            and
              predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: warning
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
            summary: Filesystem is predicted to run out of space within the next 4 hours.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 10
            and
              predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: critical
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
            summary: Filesystem has less than 5% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 30m
          labels:
            severity: warning
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
            summary: Filesystem has less than 3% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 30m
          labels:
            severity: critical
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
            summary: Filesystem is predicted to run out of inodes within the next 24 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 40
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: warning
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
            summary: Filesystem is predicted to run out of inodes within the next 4 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 20
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: critical
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
            summary: Filesystem has less than 5% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: warning
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
            summary: Filesystem has less than 3% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
            )
          for: 1h
          labels:
            severity: critical
        - alert: NodeNetworkReceiveErrs
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
            summary: Network interface is reporting many receive errors.
          expr: |
            rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
          for: 1h
          labels:
            severity: warning
        - alert: NodeNetworkTransmitErrs
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
            summary: Network interface is reporting many transmit errors.
          expr: |
            rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
          for: 1h
          labels:
            severity: warning
        - alert: NodeHighNumberConntrackEntriesUsed
          annotations:
            description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
            summary: Number of conntrack are getting close to the limit.
          expr: |
            (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
          labels:
            severity: warning
        - alert: NodeTextFileCollectorScrapeError
          annotations:
            description: Node Exporter text file collector on {{ $labels.instance }} failed to scrape.
            summary: Node Exporter text file collector failed to scrape.
          expr: |
            node_textfile_scrape_error{job="node-exporter"} == 1
          labels:
            severity: warning
        - alert: NodeClockSkewDetected
          annotations:
            description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
            summary: Clock skew detected.
          expr: |
            (
              node_timex_offset_seconds{job="node-exporter"} > 0.05
            and
              deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
            )
            or
            (
              node_timex_offset_seconds{job="node-exporter"} < -0.05
            and
              deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
            )
          for: 10m
          labels:
            severity: warning
        - alert: NodeClockNotSynchronising
          annotations:
            description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
            summary: Clock not synchronising.
          expr: |
            min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
            and
            node_timex_maxerror_seconds{job="node-exporter"} >= 16
          for: 10m
          labels:
            severity: critical
        - alert: NodeRAIDDegraded
          annotations:
            description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md
            summary: RAID Array is degraded.
          expr: |
            node_md_disks_required{job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}) > 0
          for: 15m
          labels:
            severity: critical
        - alert: NodeRAIDDiskFailure
          annotations:
            description: At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
            summary: Failed device in RAID array.
          expr: |
            node_md_disks{state="failed",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} > 0
          labels:
            severity: warning
        - alert: NodeFileDescriptorLimit
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |
            (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
            )
          for: 15m
          labels:
            severity: warning
        - alert: NodeFileDescriptorLimit
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |
            (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
            )
          for: 15m
          labels:
            severity: critical
        - alert: NodeSystemSaturation
          annotations:
            description: |
              System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
              This might indicate this instance resources saturation and can cause it becoming unresponsive.
            summary: System saturated, load per core is very high.
          expr: |
            node_load1{job="node-exporter"}
            / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
          for: 15m
          labels:
            severity: warning
        - alert: NodeMemoryMajorPagesFaults
          annotations:
            description: |
              Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
              Please check that there is enough memory available at this instance.
            summary: Memory major page faults are occurring at very high rate.
          expr: |
            rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
          for: 15m
          labels:
            severity: warning
        - alert: NodeSystemdServiceFailed
          annotations:
            description: Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}
            summary: Systemd service has entered failed state.
          expr: |
            node_systemd_unit_state{job="node-exporter", state="failed"} == 1
          for: 15m
          labels:
            severity: warning
    - name: node-exporter.rules
      rules:
        - expr: |
            count without (cpu, mode) (
              node_cpu_seconds_total{job="node-exporter",mode="idle"}
            )
          record: instance:node_num_cpu:sum
        - expr: |
            1 - avg without (cpu) (
              sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[1m]))
            )
          record: instance:node_cpu_utilisation:rate1m
        - expr: |
            (
              node_load1{job="node-exporter"}
            /
              instance:node_num_cpu:sum{job="node-exporter"}
            )
          record: instance:node_load1_per_cpu:ratio
        - expr: |
            1 - (
              (
                node_memory_MemAvailable_bytes{job="node-exporter"}
                or
                (
                  node_memory_Buffers_bytes{job="node-exporter"}
                  +
                  node_memory_Cached_bytes{job="node-exporter"}
                  +
                  node_memory_MemFree_bytes{job="node-exporter"}
                  +
                  node_memory_Slab_bytes{job="node-exporter"}
                )
              )
            /
              node_memory_MemTotal_bytes{job="node-exporter"}
            )
          record: instance:node_memory_utilisation:ratio
        - expr: |
            rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
          record: instance:node_vmstat_pgmajfault:rate1m
        - expr: |
            rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
          record: instance_device:node_disk_io_time_seconds:rate1m
        - expr: |
            rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
          record: instance_device:node_disk_io_time_weighted_seconds:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_drop_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_drop_excluding_lo:rate1m
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-prometheus-k8s-prometheus-rules
spec:
  groups:
    - name: prometheus
      rules:
        - alert: PrometheusBadConfig
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
            summary: Failed Prometheus configuration reload.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_config_last_reload_successful{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) == 0
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusSDRefreshFailure
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.
            summary: Failed Prometheus SD refresh.
          expr: |
            increase(prometheus_sd_refresh_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[10m]) > 0
          for: 20m
          labels:
            severity: warning
        - alert: PrometheusNotificationQueueRunningFull
          annotations:
            description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
            summary: Prometheus alert notification queue predicted to run full in less than 30m.
          expr: |
            # Without min_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              predict_linear(prometheus_notifications_queue_length{job=~"prometheus-k8s|prometheus-user-workload"}[5m], 60 * 30)
            >
              min_over_time(prometheus_notifications_queue_capacity{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            )
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
          annotations:
            description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
            summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
          expr: |
            (
              rate(prometheus_notifications_errors_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            /
              rate(prometheus_notifications_sent_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            )
            * 100
            > 1
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusNotConnectedToAlertmanagers
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
            summary: Prometheus is not connected to any Alertmanagers.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_notifications_alertmanagers_discovered{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) < 1
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusTSDBReloadsFailing
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.
            summary: Prometheus has issues reloading blocks from disk.
          expr: |
            increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
          for: 4h
          labels:
            severity: warning
        - alert: PrometheusTSDBCompactionsFailing
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
            summary: Prometheus has issues compacting blocks.
          expr: |
            increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
          for: 4h
          labels:
            severity: warning
        - alert: PrometheusNotIngestingSamples
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
            summary: Prometheus is not ingesting samples.
          expr: |
            (
              rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) <= 0
            and
              (
                sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
              or
                sum without(rule_group) (prometheus_rule_group_rules{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
              )
            )
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusDuplicateTimestamps
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
            summary: Prometheus is dropping samples with duplicate timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 1h
          labels:
            severity: warning
        - alert: PrometheusOutOfOrderTimestamps
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
            summary: Prometheus drops samples with out-of-order timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_out_of_order_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 1h
          labels:
            severity: warning
        - alert: PrometheusRemoteStorageFailures
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
            summary: Prometheus fails to send samples to remote storage.
          expr: |
            (
              (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
            /
              (
                (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
              +
                (rate(prometheus_remote_storage_succeeded_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
              )
            )
            * 100
            > 1
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusRemoteWriteBehind
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
            summary: Prometheus remote write is behind.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            - ignoring(remote_name, url) group_right
              max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            )
            > 120
          for: 15m
          labels:
            severity: info
        - alert: PrometheusRemoteWriteDesiredShards
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job=~"prometheus-k8s|prometheus-user-workload"}` $labels.instance | query | first | value }}.
            summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              max_over_time(prometheus_remote_storage_shards_desired{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            >
              max_over_time(prometheus_remote_storage_shards_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            )
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusRuleFailures
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md
            summary: Prometheus is failing rule evaluations.
          expr: |
            increase(prometheus_rule_evaluation_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusMissingRuleEvaluations
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
            summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
          expr: |
            increase(prometheus_rule_group_iterations_missed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusTargetLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded the configured target_limit.
            summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
          expr: |
            increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusLabelLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
            summary: Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
          expr: |
            increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusScrapeBodySizeLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.
            summary: Prometheus has dropped some targets that exceeded body size limit.
          expr: |
            increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusScrapeSampleLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.
            summary: Prometheus has failed scrapes that have exceeded the configured sample limit.
          expr: |
            increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusTargetSyncFailure
          annotations:
            description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.'
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md
            summary: Prometheus has failed to sync targets.
          expr: |
            increase(prometheus_target_sync_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[30m]) > 0
          for: 5m
          labels:
            severity: critical
        - alert: PrometheusHighQueryLoad
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.
            summary: Prometheus is reaching its maximum capacity serving concurrent requests.
          expr: |
            avg_over_time(prometheus_engine_queries{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0.8
          for: 15m
          labels:
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-prometheus-k8s-thanos-sidecar-rules
spec:
  groups:
    - name: thanos-sidecar
      rules:
        - alert: ThanosSidecarBucketOperationsFailed
          annotations:
            description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} bucket operations are failing
            summary: Thanos Sidecar bucket operations are failing
          expr: |
            sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"}[5m])) > 0
          for: 1h
          labels:
            severity: warning
        - alert: ThanosSidecarNoConnectionToStartedPrometheus
          annotations:
            description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} is unhealthy.
            summary: Thanos Sidecar cannot access Prometheus, even though Prometheus seems healthy and has reloaded WAL.
          expr: |
            thanos_sidecar_prometheus_up{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"} == 0
            AND on (namespace, pod)
            prometheus_tsdb_data_replay_duration_seconds != 0
          for: 1h
          labels:
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-prometheus-operator-rules
spec:
  groups:
    - name: prometheus-operator
      rules:
        - alert: PrometheusOperatorListErrors
          annotations:
            description: Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
            summary: Errors while performing list operations in controller.
          expr: |
            (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m]))) > 0.4
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusOperatorWatchErrors
          annotations:
            description: Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
            summary: Errors while performing watch operations in controller.
          expr: |
            (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.4
          for: 15m
          labels:
            severity: warning
        - alert: PrometheusOperatorSyncFailed
          annotations:
            description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
            summary: Last controller reconciliation failed
          expr: |
            min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusOperatorReconcileErrors
          annotations:
            description: '{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
            summary: Errors while reconciling controller.
          expr: |
            (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusOperatorNodeLookupErrors
          annotations:
            description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
            summary: Errors while reconciling Prometheus.
          expr: |
            rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusOperatorNotReady
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
            summary: Prometheus operator not ready
          expr: |
            min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) == 0)
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusOperatorRejectedResources
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
            summary: Resources rejected by Prometheus operator
          expr: |
            min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
          for: 5m
          labels:
            severity: warning
    - name: config-reloaders
      rules:
        - alert: ConfigReloaderSidecarErrors
          annotations:
            description: |-
              Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
              As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
            summary: config-reloader sidecar has not had a successful reload for 10m
          expr: |
            max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
          for: 10m
          labels:
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-rhmi-sre-cluster-admins
spec:
  groups:
    - name: rhmi-sre-cluster-admins
      rules:
        - alert: ElevatingClusterAdminRHMISRE
          annotations:
            message: RHMI SRE "{{ $labels.user }}" elevated to cluster-admin({{ $labels.group }}) more than 2 hours.
          expr: openshift_group_user_account{group="layered-sre-cluster-admins"} == 1
          for: 130m
          labels:
            namespace: redhat-rhmi
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-rhoam-sre-cluster-admins
spec:
  groups:
    - name: rhoam-sre-cluster-admins
      rules:
        - alert: ElevatingClusterAdminRHOAMSRE
          annotations:
            message: RHOAM SRE "{{ $labels.user }}" elevated to cluster-admin({{ $labels.group }}) more than 2 hours.
          expr: openshift_group_user_account{group="rhoam-sre-cluster-admins"} == 1
          for: 130m
          labels:
            namespace: redhat-rhoam
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-alertmanager-silences-active
spec:
  groups:
    - name: sre-alertmanager-silences-active
      rules:
        - alert: AlertmanagerSilencesActiveSRE
          annotations:
            message: Active AlertManager silences have been detected on the cluster for the last 15 minutes. As a result, active alerts may potentially not be being reported back.
          expr: avg without (instance,pod,endpoint,job,service,state,container)(alertmanager_silences{state="active"}) > 0 unless (count(cluster_version{type="updating"}) > 0 or sum by(namespace) (time() - kube_pod_created{namespace="openshift-monitoring",pod=~"osd-cluster-ready.*"} < 4500))
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-cannot-retrieve-updates-alerts
spec:
  groups:
    - name: sre-cannot-retrieve-updates
      rules:
        - alert: CannotRetrieveUpdatesSRE
          annotations:
            description: Failure to retrieve updates means that cluster administrators will need to monitor for available updates on their own or risk falling behind on security or other bugfixes. If the failure is expected, you can clear spec.channel in the ClusterVersion object to tell the cluster-version operator to not retrieve updates. Cluster version operator has not retrieved updates in {{ $value | humanizeDuration }}. Failure reason {{ with $cluster_operator_conditions := "cluster_operator_conditions" | query}}{{range $value := .}}{{if and (eq (label "name" $value) "version") (eq (label "condition" $value) "RetrievedUpdates") (eq (label "endpoint" $value) "metrics") (eq (value $value) 0.0)}}{{label "reason" $value}} {{end}}{{end}}{{end}}. {{ with $console_url := "console_url" | query }}{{ if ne (len (label "url" (first $console_url ) ) ) 0}} For more information refer to {{ label "url" (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}
            summary: Cluster version operator has not retrieved updates.
          expr: ' (time()-cluster_version_operator_update_retrieval_timestamp_seconds) >= 3600 and ignoring(condition, name, reason) (cluster_operator_conditions{name="version",condition="RetrievedUpdates", endpoint="metrics", reason!="NoChannel"}) and on(instance, job, namespace, pod, service) (cluster_version{version!~"(4\\.11\\.36|4\\.12\\.12)"}) '
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-cluster-monitoring-error-budget-burn
spec:
  groups:
    - name: sre-cluster-monitoring-error-budget-burn
      rules:
        - alert: ClusterMonitoringErrorBudgetBurnSRE
          annotations:
            message: 'High error budget burn for the monitoring cluster operator (current value: {{ $value }})'
          expr: |
            1 - ( sum(sum_over_time(cluster_operator_up{name="monitoring"}[5m])) / sum(count_over_time(cluster_operator_up{name="monitoring"}[5m])) ) > (14.4 * (1 - 0.983))
            and
            1 - ( sum(sum_over_time(cluster_operator_up{name="monitoring"}[1h])) / sum(count_over_time(cluster_operator_up{name="monitoring"}[1h])) ) > (14.4 * (1 - 0.983))
          for: 2m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/troubleshoot/clusteroperators/monitoring.md
            long_window: 1h
            namespace: openshift-monitoring
            severity: critical
            short_window: 5m
            source: https://issues.redhat.com/browse/OSD-19769
        - alert: ClusterMonitoringErrorBudgetBurnSRE
          annotations:
            message: 'High error budget burn for the monitoring cluster operator (current value: {{ $value }})'
          expr: |
            1 - ( sum(sum_over_time(cluster_operator_up{name="monitoring"}[30m])) / sum(count_over_time(cluster_operator_up{name="monitoring"}[30m])) ) > (6 * (1 - 0.983))
            and
            1 - ( sum(sum_over_time(cluster_operator_up{name="monitoring"}[6h])) / sum(count_over_time(cluster_operator_up{name="monitoring"}[6h])) ) > (6 * (1 - 0.983))
          for: 15m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/troubleshoot/clusteroperators/monitoring.md
            long_window: 6h
            namespace: openshift-monitoring
            severity: critical
            short_window: 30m
            source: https://issues.redhat.com/browse/OSD-19769
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-cluster-version-operator
spec:
  groups:
    - name: sre-cluster-version-operator
      rules:
        - alert: ClusterOperatorDegradedSRE
          annotations:
            description: The {{ "{{ $labels.name }}" }} operator is degraded because {{ "{{ $labels.reason }}" }}, and the components it manages may have reduced quality of service.  Cluster upgrades may not complete. For more information refer to 'oc get -o yaml clusteroperator {{ "{{ $labels.name }}" }}'{{ "{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}" }}.
            summary: Cluster operator has been degraded for 1 day.
          expr: |
            max by (namespace, name, reason)
            (
              (
                cluster_operator_conditions{job="cluster-version-operator", condition="Degraded", name =~ "kube-apiserver|kube-controller-manager|kube-scheduler"}
                or on (namespace, name)
                group by (namespace, name) (cluster_operator_up{job="cluster-version-operator", name =~ "kube-apiserver|kube-controller-manager|kube-scheduler"})
              ) == 1
            )
          for: 1d
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ClusterOperatorDegraded.md
            namespace: openshift-monitoring
            needs_attention: "true"
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-configure-alertmanager-operator-offline-alerts
spec:
  groups:
    - name: sre-configure-alertmanager-operator-offline-alerts
      rules:
        - alert: ConfigureAlertmanagerOperatorOfflineSRE
          expr: absent(up{service="configure-alertmanager-operator"})
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-control-plane-resizing-alerts
spec:
  groups:
    - name: sre-control-plane-resizing-recording.rules
      rules:
        - expr: label_replace(cluster:nodes_roles, "instance", "$1", "node", "(.*)") * on(instance) group_left node_memory_MemTotal_bytes
          record: sre:node_roles:memory_total_bytes
        - expr: ( avg ( avg by (instance) ( sum by (cpu, instance) ( rate( node_cpu_seconds_total{mode!="idle"}[8h] ) ) ) * on (instance) ( label_replace ( kube_node_role{role ="master"}, "instance", "$1", "node", "(.*)" ) ) ) > ( scalar ( ( count ( cluster:nodes_roles{label_node_role_kubernetes_io ="master"} ) - 1 ) / count ( cluster:nodes_roles{label_node_role_kubernetes_io ="master"} ) ) ) )
          record: sre:node_control_plane:excessive_consumption_cpu
        - expr: ( 1 - sum ( node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" ) ) / sum ( node_memory_MemTotal_bytes AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" ) ) ) > ( scalar ( ( count ( cluster:nodes_roles{label_node_role_kubernetes_io ="master"} ) - 1 ) / count ( cluster:nodes_roles{label_node_role_kubernetes_io ="master"} ) ) )
          record: sre:node_control_plane:excessive_consumption_memory
    - name: sre-control-plane-resizing-alerts
      rules:
        - alert: ControlPlaneNodesNeedResizingSRE
          annotations:
            message: The cluster's control plane nodes have been undersized for 15 minutes and should be vertically scaled to support the existing workload. See linked SOP for details. Critical alert will be raised at 24 hours.
          expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: ControlPlaneNodesNeedResizingSRE
          annotations:
            message: The cluster's control plane nodes have been undersized for 24 hours and must be vertically scaled to support the existing workload. See linked SOP for details.
          expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
          for: 24h
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-elasticsearch-jobs
spec:
  groups:
    - name: sre-elasticsearch-jobs
      rules:
        - alert: ElasticsearchJobFailedSRE
          annotations:
            message: Elasticsearch job {{ $labels.job_name }} has failed 2 or more times in the last 15 minutes in namespace {{ $labels.namespace }}.
          expr: kube_job_failed{job="kube-state-metrics",namespace="openshift-logging",job_name=~"^elasticsearch.*"} > 1
          for: 15m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-elasticsearch-managed-notification-alerts
spec:
  groups:
    - name: sre-elasticsearch-managed-notification-alerts
      rules:
        - alert: ElasticsearchNodeDiskWatermarkReachedNotificationSRE
          expr: count(ALERTS{alertname="ElasticsearchNodeDiskWatermarkReached", alertstate="firing", namespace="openshift-logging", severity="critical"}) >= 1
          for: 30m
          labels:
            managed_notification_template: ElasticsearchNodeDiskWatermarkReached
            namespace: openshift-logging
            send_managed_notification: "true"
            severity: Info
        - alert: ElasticsearchDiskSpaceRunningLowNotificationSRE
          expr: count(ALERTS{alertname="ElasticsearchDiskSpaceRunningLow", alertstate="firing", namespace="openshift-logging"}) >= 1
          for: 30m
          labels:
            managed_notification_template: ElasticsearchDiskSpaceRunningLow
            namespace: openshift-logging
            send_managed_notification: "true"
            severity: Info
        - alert: ElasticsearchClusterNotHealthyNotificationSRE
          expr: count(ALERTS{alertname="ElasticsearchClusterNotHealthy", alertstate="firing", namespace="openshift-logging", severity="critical"}) >= 1
          for: 10m
          labels:
            managed_notification_template: ElasticsearchClusterNotHealthy
            namespace: openshift-logging
            send_managed_notification: "true"
            severity: Info
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-excessive-memory
spec:
  groups:
    - name: sre-excessive-memory
      rules:
        - alert: ExcessiveContainerMemoryWarningSRE
          annotations:
            message: System container {{ $labels.namespace }}/{{ $labels.pod_name }}/{{ $labels.container_name }} is using in excess of 3G of memory for over 30 minutes.
          expr: container_memory_rss{namespace=~"(^openshift.*|^kube.*|^default$)",namespace!="openshift-customer-monitoring",container_name!="",container_name!="POD",pod!~"(^prometheus-k8s-.*|^elasticsearch-.*)"}/1024/1024/1024>3
          for: 30m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
        - alert: ExcessiveContainerMemoryCriticalSRE
          annotations:
            message: System container {{ $labels.namespace }}/{{ $labels.pod_name }}/{{ $labels.container_name }} is using in excess of 5G of memory for over 30 minutes.
          expr: container_memory_rss{namespace=~"(^openshift.*|^kube.*|^default$)",namespace!="openshift-customer-monitoring",container_name!="",container_name!="POD",pod!~"(^prometheus-k8s-.*|^elasticsearch-.*)"}/1024/1024/1024>5
          for: 30m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-haproxy-reload-fail
spec:
  groups:
    - name: sre-haproxy-reload-fails
      rules:
        - alert: HAProxyReloadFailSRE
          annotations:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/HAProxyReloadFailSRE.md
            message: HAProxy reloads have failed on {{ $labels.pod }}. Router is not respecting recently created or modified routes
          expr: increase(template_router_reload_fails[5m]) > 0
          for: 15m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-haproxydown-default
spec:
  groups:
    - name: sre-haproxydown-alerts
      rules:
        - alert: HAProxyDownSRE
          annotations:
            description: This alert fires when metrics report that HAProxy is down.
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/HAProxyDown.md
            message: HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace }} / {{ $labels.pod }}
            summary: HAProxy is down
          expr: haproxy_up{job="router-internal-default"} == 0
          for: 5m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-infra-resizing-alerts
spec:
  groups:
    - name: sre-infra-resource-consumption-recording.rules
      rules:
        - expr: ( avg ( avg by (instance) ( sum by (cpu, instance) ( rate( node_cpu_seconds_total{mode!="idle"}[1h] ) ) ) * on (instance) ( label_replace ( kube_node_role{role ="infra"}, "instance", "$1", "node", "(.*)" ) ) ) > ( scalar ( ( count ( cluster:nodes_roles{label_node_role_kubernetes_io ="infra"} ) - 1 ) / count ( cluster:nodes_roles{label_node_role_kubernetes_io ="infra"} ) ) ) )
          record: sre:node_infra:excessive_consumption_cpu
        - expr: ( 1 - sum ( node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes AND on (instance) label_replace( kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)" ) ) / sum ( node_memory_MemTotal_bytes AND on (instance) label_replace( kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)" ) ) ) > ( scalar ( ( count ( cluster:nodes_roles{label_node_role_kubernetes_io ="infra"} ) - 1 ) / count ( cluster:nodes_roles{label_node_role_kubernetes_io ="infra"} ) ) )
          record: sre:node_infra:excessive_consumption_memory
        - expr: ( count( ALERTS{alertname="cpu-InfraNodesExcessiveResourceConsumptionSRE1h", alertstate="firing"} OR ALERTS{alertname="memory-InfraNodesExcessiveResourceConsumptionSRE", alertstate="firing"} ) >= 1 )
          record: sre:node_infras:need_resize
    - name: sre-infra-resizing-alerts
      rules:
        - alert: cpu-InfraNodesExcessiveResourceConsumptionSRE1h
          annotations:
            message: The cluster's infrastructure nodes have been consuming excessive CPU for 1 hours and may need to be vertically scaled to support the existing workers. See linked SOP for details.
          expr: sre:node_infra:excessive_consumption_cpu > 0
          for: 1h
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: cpu-InfraNodesExcessiveResourceConsumptionSRE
          annotations:
            message: The cluster's infrastructure nodes have been consuming excessive CPU for 16 hours and may need to be vertically scaled to support the existing workers. See linked SOP for details.
          expr: sre:node_infra:excessive_consumption_cpu > 0
          for: 16h
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: memory-InfraNodesExcessiveResourceConsumptionSRE
          annotations:
            message: The cluster's infrastructure nodes have been consuming excessive memory for 24 hours and may need to be vertically scaled to support the existing workers. See linked SOP for details.
          expr: sre:node_infra:excessive_consumption_memory > 0
          for: 24h
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: InfraNodesNeedResizingSRE
          annotations:
            message: The cluster's infrastructure nodes have been undersized for 2 hours and must be vertically scaled to support the existing workers.  See linked SOP for details.
          expr: sre:node_infras:need_resize > 0
          for: 5m
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-insights-operator-alerts
spec:
  groups:
    - name: sre-insights-operator-alerts
      rules:
        - alert: InsightsOperatorDownSRE
          annotations:
            description: The {{ "{{ $labels.name }}" }} operator may be down or disabled because {{ "{{ $labels.reason }}" }}, and the components it manages may be unavailable or degraded.  Cluster upgrades may not complete. For more information refer to 'oc get -o yaml clusteroperator {{ "{{ $labels.name }}" }}'{{ "{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}" }}.
            link: https://github.com/openshift/ops-sop/blob/master/v4/troubleshoot/clusteroperators/insights.md
            summary: Insights operator has not been available for 3 hours.
          expr: |
            max by (namespace, name, reason) (cluster_operator_up{job="cluster-version-operator", name="insights"} == 0)
          for: 3h
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: recording-rules
  name: user-workload-sre-internal-slo-recording-rules
spec:
  groups:
    - name: sre-internal-slo.rules
      rules:
        - expr: sre:telemetry:managed_labels * on (version) group_left(alerts, upgradeconfig_name) label_replace(upgradeoperator_upgrade_result, "sre", "true", "", "")
          record: sre:slo:upgradeoperator_upgrade_result
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-kubejobfailing
spec:
  groups:
    - name: sre-kubejobfailing-recording.rules
      rules:
        - expr: |
            clamp_max(job:kube_job_status_start_time:max, 1)
            * ON(job_name) GROUP_LEFT()
            (kube_job_status_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0)
          record: job:kube_job_status_failed:sum
        - expr: "max(\n kube_job_status_start_time{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n * ON(job_name,namespace) GROUP_RIGHT()\n kube_job_owner{owner_name!=\"\",owner_name!=\"<none>\"}\n)   \nBY (job_name, owner_name, namespace)\n== ON(owner_name) GROUP_LEFT()\nmax(\n kube_job_status_start_time{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n * ON(job_name,namespace) GROUP_RIGHT()\n kube_job_owner{owner_name!=\"\",owner_name!=\"<none>\"}\n)   \nBY (owner_name)\n"
          record: job:kube_job_status_start_time:max
    - name: sre-kubejobfailing
      rules:
        - alert: KubeJobFailingSRE
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is failing to complete.
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/KubeJobFailed.md
            summary: Job is failing to complete.
          expr: |
            job:kube_job_status_failed:sum
          for: 15m
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-kubequotaexceeded
spec:
  groups:
    - name: sre-kubequotaexceeded
      rules:
        - alert: KubeQuotaExceededSRE
          annotations:
            message: Quota limit exceeded in namespace {{ $labels.namespace }}.
          expr: |
            kube_resourcequota{namespace=~"(^kube$|^kube-.*|^openshift$|^openshift-.*|^default$)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
            (kube_resourcequota{namespace=~"(^kube$|^kube-.*|^openshift$|^openshift-.*|^default$)",job="kube-state-metrics", type="hard"} > 0)
            > 1
          for: 30m
          labels:
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-leader-election-master-status-alerts
spec:
  groups:
    - name: sre-leader-election-master-status-alerts
      rules:
        - alert: ControlPlaneLeaderElectionFailingSRE
          annotations:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ControlPlaneLeaderElectionFailingSRE.md
            message: Control plane has failing leader election for 10 minutes and should be scaled to support cluster.
          expr: sum(leader_election_master_status{container=~"kube-scheduler|provisioner-kube-rbac-proxy|kube-controller-manager" }) by (container) < 1
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-kube-apiserver-missing-on-node
spec:
  groups:
    - name: sre-managed-kube-apiserver-missing-on-node
      rules:
        - alert: KubeAPIServerMissingOnNode60Minutes
          annotations:
            message: Static pod kube-apiserver is not running on node {{ $labels.node }} for 60 minutes.
          expr: |
            count(cluster:master_nodes{}) by (node) unless count(kube_pod_info{namespace="openshift-kube-apiserver", pod=~"kube-apiserver-.*", pod!~".*guard.*"}) by (node) >= 1
          for: 60m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/StaticPodMissing.md
            maturity: immature
            severity: warning
            source: https://issues.redhat.com/browse/OSD-13647
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-kube-controller-manager-crashlooping
spec:
  groups:
    - name: sre-managed-kube-controller-manager-crashlooping
      rules:
        - alert: KubeControllerManagerCrashloopingSRE
          annotations:
            message: Static pod kube-controller-manager pods have been crashlooping for 60 minutes.
          expr: |
            sum(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace="openshift-kube-controller-manager",pod=~"kube-controller-manager-.*",job="kube-state-metrics"}) >= 2
          for: 60m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/KubeControllerManagerCrashloopingSRE.md
            severity: critical
            source: https://issues.redhat.com/browse/OSD-15442
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-kube-controller-manager-missing-on-node
spec:
  groups:
    - name: sre-managed-kube-controller-manager-missing-on-node
      rules:
        - alert: KubeControllerManagerMissingOnNode60Minutes
          annotations:
            message: Static pod kube-controller-manager is not running on node {{ $labels.node }} for 60 minutes.
          expr: |
            count(cluster:master_nodes{}) by (node) unless count(kube_pod_info{namespace="openshift-kube-controller-manager", pod=~"kube-controller-manager-.*", pod!~".*guard.*"}) by (node) >= 1
          for: 60m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/StaticPodMissing.md
            severity: critical
            source: https://issues.redhat.com/browse/OSD-13647
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-kube-scheduler-missing-on-node
spec:
  groups:
    - name: sre-managed-kube-scheduler-missing-on-node
      rules:
        - alert: KubeSchedulerMissingOnNode60Minutes
          annotations:
            message: Static pod kube-scheduler is not running on node {{ $labels.node }} for 60 minutes.
          expr: |
            count(cluster:master_nodes{}) by (node) unless count(kube_pod_info{namespace="openshift-kube-scheduler", pod=~"openshift-kube-scheduler-.*", pod!~".*guard.*"}) by (node) >= 1
          for: 60m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/StaticPodMissing.md
            maturity: immature
            severity: warning
            source: https://issues.redhat.com/browse/OSD-13647
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-node-metadata-operator-alerts
spec:
  groups:
    - name: sre-managed-node-metadata-operator-alerts
      rules:
        - alert: MNMOTooManyReconcileErrors15MinSRE
          annotations:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/MNMOTooManyReconcileFailures.md
            message: Reconciliations of the MNMO operator ( {{ $labels.name }} ) have failed in the past 15 minutes.
          expr: increase(controller_runtime_reconcile_total{controller="machineset_controller", service="managed-node-metadata-operator-metrics-service", result="error"}[20m])>0
          for: 15m
          labels:
            maturity: immature
            namespace: '{{ $labels.namespace }}'
            severity: warning
            source: https://issues.redhat.com//browse/OSD-9911
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-notification-alerts
spec:
  groups:
    - name: sre-managed-notification-alerts
      rules:
        - alert: KubePersistentVolumeFillingUpSRE
          expr: count by (namespace, persistentvolumeclaim) (ALERTS{alertname="KubePersistentVolumeFillingUp", alertstate="firing", namespace="openshift-user-workload-monitoring"}) >= 1
          for: 30m
          labels:
            managed_notification_template: KubePersistentVolumeFillingUp
            namespace: '{{ $labels.namespace }}'
            send_managed_notification: "true"
            severity: Info
        - alert: LoggingVolumeFillingUpNotificationSRE
          expr: count(ALERTS{alertname="KubePersistentVolumeFillingUp", alertstate="firing", namespace="openshift-logging"}) >= 1
          for: 30m
          labels:
            managed_notification_template: LoggingVolumeFillingUp
            namespace: openshift-logging
            send_managed_notification: "true"
            severity: Info
        - alert: MultipleDefaultStorageClassesNotificationSRE
          expr: count(ALERTS{alertname="MultipleDefaultStorageClasses", alertstate="firing", namespace="openshift-cluster-storage-operator"}) >= 1
          for: 30m
          labels:
            managed_notification_template: MultipleDefaultStorageClasses
            namespace: openshift-cluster-storage-operator
            send_managed_notification: "true"
            severity: Info
        - alert: NonSystemChangeValidatingWebhookConfigurationsNotificationSRE
          expr: sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m]) or vector(0)) - sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m] offset 5m) or vector(0)) > 0
          labels:
            managed_notification_template: NonSystemChangeValidationWebhookConfigurations
            namespace: audit-exporter
            send_managed_notification: "true"
            severity: Info
        - alert: WorkerNodeFileDescriptorLimitSRE
          annotations:
            message: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |-
            group by (instance) (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} >= 90
            )
            * on (instance) group_left ()group by(instance) (
              label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
            unless
            group by(instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 15m
          labels:
            managed_notification_template: WorkerNodeFileDescriptorAtLimit
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: critical
        - alert: WorkerNodeFilesystemSpaceFillingUp
          annotations:
            message: Filesystem is predicted to run out of inodes within the next 4 hours.
          expr: |-
            (node_filesystem_files_free{fstype!="",job="node-exporter"} / node_filesystem_files{fstype!="",job="node-exporter"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!="",job="node-exporter"} == 0)
            * on (instance) group_left ()group by(instance) (
              label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
            unless
            group by(instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 1h
          labels:
            managed_notification_template: WorkerNodeFilesystemSpaceFillingUp
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: critical
        - alert: CustomerWorkloadPreventingDrainSRE
          annotations:
            message: A non-openshift workload is preventing a node from draining.
          expr: |-
            (
              (
                kube_node_spec_unschedulable > 0
                unless
                ignoring(node,container,endpoint,job,namespace,service)
                sum(mapi_machine_set_status_replicas{name=~".*upgrade$"}) > 0
              )
              * on (node)
              group_right ()group by (node)
                (pods_preventing_node_drain)
            )
            * on (node)
            group_left ()group by (node)
              (kube_node_role{role!~"infra|control-plane|master"})
            unless group by(node)
              (kube_node_role{role=~"infra|control-plane|master"})
          for: 30m
          labels:
            managed_notification_template: CustomerWorkloadPreventingDrain
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: critical
        - alert: WorkerNodeFilesystemAlmostOutOfFiles
          annotations:
            message: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
            summary: Filesystem has less than 3% inodes left.
          expr: |-
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            ) * on (instance) group_left () group by (instance) (
              label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            ) unless group by(instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 1h
          labels:
            managed_notification_template: WorkerNodeFilesystemAlmostOutOfFiles
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-upgrade-operator-alerts
spec:
  groups:
    - name: sre-managed-upgrade-operator-alerts
      rules:
        - alert: UpgradeConfigValidationFailedSRE
          annotations:
            description: Upgrade config validation failed
            summary: Upgrade config validation failed
          expr: upgradeoperator_upgradeconfig_validation_failed == 1
          for: 2h
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: UpgradeControlPlaneUpgradeTimeoutSRE
          annotations:
            description: controlplane upgrade for {{ $labels.version }} cannot be finished in the given time period
            summary: Controlplane upgrade timeout for {{ $labels.version }}
          expr: avg_over_time(upgradeoperator_controlplane_timeout[10m]) == 1
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: UpgradeNodeUpgradeTimeoutSRE
          annotations:
            description: nodes upgrade for {{ $labels.version }} cannot be finished after the silence expired
            summary: Nodes upgrade timeout for {{ $labels.version }}
          expr: avg_over_time(upgradeoperator_worker_timeout[10m]) == 1
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: UpgradeNodeDrainFailedSRE
          annotations:
            description: node drain takes too long and cannot be finished in the given time period during cluster upgrade
            summary: Node drain failed in the given time period which is not caused by the PDB
          expr: avg_over_time(upgradeoperator_node_drain_timeout[10m]) == 1
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: UpgradeConfigSyncFailureOver4HrSRE
          annotations:
            description: This clusters UpgradeConfig has not been synced in 4 hours and may be out of date
            summary: UpgradeConfig has not successfully synced in 4 hours.
          expr: absent_over_time(upgradeoperator_upgradeconfig_sync_timestamp[4h]) or time() - upgradeoperator_upgradeconfig_sync_timestamp > 14400
          for: 2m
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-managed-velero-operator-alerts
spec:
  groups:
    - name: sre-managed-velero-operator-alerts
      rules:
        - alert: VeleroHourlyObjectBackupsMissedConsecutively
          annotations:
            message: Consecutive hourly Velero backups have not successfully completed
          expr: |
            time() - clamp_min(velero_backup_last_successful_timestamp{namespace="openshift-velero",schedule="hourly-object-backup"},
            scalar(max(kube_deployment_created{namespace="openshift-velero",deployment="velero"}))) > 10800 + 600
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
        - alert: VeleroDailyFullBackupMissed
          annotations:
            message: The daily Velero backup has not successfully completed
          expr: |
            time() - clamp_min(velero_backup_last_successful_timestamp{namespace="openshift-velero",schedule="daily-full-backup"},
            scalar(max(kube_deployment_created{namespace="openshift-velero",deployment="velero"}))) > 86400 + 600
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
        - alert: VeleroWeeklyFullBackupMissed
          annotations:
            message: The weekly Velero backup has not successfully completed
          expr: |
            time() - clamp_min(velero_backup_last_successful_timestamp{namespace="openshift-velero",schedule="weekly-full-backup"},
            scalar(max(kube_deployment_created{namespace="openshift-velero",deployment="velero"}))) > 604800 + 600
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-node-condition-managed-notification-alerts
spec:
  groups:
    - name: sre-node-condition-managed-notification-alerts.record
      rules:
        - expr: label_replace ( clamp(mapi_machine_created_timestamp_seconds, 0, 1) * on (node) group_right(name) kube_node_status_condition{status!="false"}>0, "machineset", "$1", "name", "^(.*)-[^-]*$" )
          record: sre:node:condition
    - name: sre-node-condition-managed-notification-alerts.alert
      rules:
        - alert: NodeConditionDiskPressureNotificationSRE
          expr: count by (machineset, condition, status) ( count by(node) (cluster:cpu_core_node_labels{label_node_role_kubernetes_io_infra="",label_node_role_kubernetes_io_master=""}) * on (node) group_left(condition, machineset, status) sre:node:condition{condition="DiskPressure"} ) > 0
          for: 5m
          labels:
            managed_notification_template: NodeConditionDiskPressureNotification
            namespace: openshift-node
            send_managed_notification: "true"
            severity: info
        - alert: NodeConditionMemoryPressureNotificationSRE
          expr: count by (machineset, condition, status) ( count by(node) (cluster:cpu_core_node_labels{label_node_role_kubernetes_io_infra="",label_node_role_kubernetes_io_master=""}) * on (node) group_left(condition, machineset, status) sre:node:condition{condition="MemoryPressure"} ) > 0
          for: 5m
          labels:
            managed_notification_template: NodeConditionMemoryPressureNotification
            namespace: openshift-node
            send_managed_notification: "true"
            severity: info
        - alert: NodeConditionPIDPressureNotificationSRE
          expr: count by (machineset, condition, status) ( count by(node) (cluster:cpu_core_node_labels{label_node_role_kubernetes_io_infra="",label_node_role_kubernetes_io_master=""}) * on (node) group_left(condition, machineset, status) sre:node:condition{condition="PIDPressure"} ) > 0
          for: 5m
          labels:
            managed_notification_template: NodeConditionPIDPressureNotification
            namespace: openshift-node
            send_managed_notification: "true"
            severity: info
        - alert: NodeConditionNetworkUnavailableNotificationSRE
          expr: count by (machineset, condition, status) ( count by(node) (cluster:cpu_core_node_labels{label_node_role_kubernetes_io_infra="",label_node_role_kubernetes_io_master=""}) * on (node) group_left(condition, machineset, status) sre:node:condition{condition="NetworkUnavailable"} ) > 0
          for: 5m
          labels:
            managed_notification_template: NodeConditionNetworkUnavailableNotification
            namespace: openshift-node
            send_managed_notification: "true"
            severity: info
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-node-filedescriptor-limit
spec:
  groups:
    - name: sre-controlplane-node-filedescriptor-limit
      rules:
        - alert: ControlPlaneNodeFileDescriptorLimitSRE
          annotations:
            message: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |-
            group by (instance) (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} >= 90
            )
            * on (instance) group_left ()group by (instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: critical
    - name: sre-controlplane-node-filesystem-space-filling-upstream
      rules:
        - alert: ControlPlaneNodeFilesystemSpaceFillingUp
          annotations:
            message: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
            summary: Filesystem is predicted to run out of inodes within the next 4 hours.
          expr: |-
            (node_filesystem_files_free{fstype!="",job="node-exporter"} / node_filesystem_files{fstype!="",job="node-exporter"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!="",job="node-exporter"} == 0)
            * on (instance) group_left ()group by (instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 1h
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: ControlPlaneNodeFilesystemAlmostOutOfFiles
          annotations:
            message: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
            summary: Filesystem has less than 3% inodes left.
          expr: |-
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            ) * on (instance) group_left () group by (instance) (
              label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
            )
          for: 1h
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-node-stuck
spec:
  groups:
    - name: sre-node-stuck
      rules:
        - alert: KubeNodeStuckWithCreatingAndTerminatingPodsSRE
          annotations:
            message: The node {{ $labels.node }} has containers stuck in creating and terminating for more than 30 minutes.
          expr: |
            sum by (node) ((
              count(
                count(kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"(openshift-.*|kube-.*|default)"}) by (pod, namespace) * on (pod, namespace) group_right kube_pod_info{}) by (node)
              )
            * on (node) group_left
              count(
                (count(kube_pod_deletion_timestamp) by (pod, namespace) * count(kube_pod_status_reason{reason="NodeLost",namespace=~"(openshift-.*|kube-.*|default)"}  == 0) by (namespace, pod) * on (pod, namespace) group_right kube_pod_info{})
              ) by (node)
            ) > 0
          for: 30m
          labels:
            maturity: immature
            namespace: openshift-monitoring
            severity: warning
            source: https://issues.redhat.com//browse/OSD-14160
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-node-unschedulable
spec:
  groups:
    - name: sre-node-unschedulable
      rules:
        - alert: KubeNodeUnschedulableSRE
          annotations:
            message: The node {{ $labels.node }} has been unschedulable for more than an hour.
          expr: (kube_node_spec_unschedulable > 0 and on(node) kube_node_role{role="master"}) or (kube_node_spec_unschedulable > 0 unless ignoring(node,container,endpoint,job,namespace,service) sum(mapi_machine_set_status_replicas{name=~".*upgrade$"}) > 0)
          for: 61m
          labels:
            namespace: openshift-monitoring
            severity: critical
        - alert: ControlPlaneNodeUnschedulableSRE
          annotations:
            message: The managed node {{ $labels.node }} has been unschedulable for more than an hour.
          expr: |-
            (
            kube_node_spec_unschedulable > 0
            unless
            ignoring(node,container,endpoint,job,namespace,service)
            sum(mapi_machine_set_status_replicas{name=~".*upgrade$"}) > 0
            )
            and on (node) (kube_node_role{role=~"infra|control-plane|master"})
          for: 60m
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: recording-rules
  name: user-workload-sre-oauth-server
spec:
  groups:
    - name: sre-oauth-server
      rules:
        - expr: apiserver_request_total{container="oauth-openshift"}
          record: oauth_server_requests_total
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-ocm-agent-operator-alerts
spec:
  groups:
    - name: sre-ocm-agent-operator-alerts
      rules:
        - alert: OCMAgentResponseFailureServiceLogsSRE
          annotations:
            message: OCM Agent has failed to receive a response from the service_logs service for 60 minutes.
          expr: avg_over_time(ocm_agent_response_failure{ocm_service="service_logs"}[60m]) == 1
          for: 60m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/OCMAgentResponseFailureServiceLogsSRE.md
            namespace: openshift-monitoring
            severity: critical
        - alert: OCMAgentOperatorPullSecretInvalidSRE
          annotations:
            message: OCM Agent Operator cannot retrieve or parse the cluster's `cloud.openshift.com` pull secret.
          expr: ocm_agent_operator_pull_secret_invalid > 0
          for: 15m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/OCMAgentResponseFailureServiceLogsSRE.md#verify-cluster-pull-secrets
            namespace: openshift-monitoring
            severity: warning
        - alert: OCMAgentPullSecretInvalidSRE
          annotations:
            message: OCM Agent pull secret auth token is not valid.
          expr: ocm_agent_pull_secret_invalid > 0
          for: 15m
          labels:
            link: https://github.com/openshift/ops-sop/blob/master/v4/alerts/OCMAgentResponseFailureServiceLogsSRE.md#verify-cluster-pull-secrets
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: recording-rules
  name: user-workload-sre-operators-recording-rules
spec:
  groups:
    - name: sre-operators.rules
      rules:
        - expr: label_replace(sre:telemetry:managed_labels, "cluster_version", "$0", "version", ".*") * on () group_right (_id, cluster_version, provider) sum by (exported_namespace, installed, namespace, name, version, channel, package) ( label_replace( csv_succeeded, "installed", "$0", "name", ".*" ) ^ on (installed) group_left (name, channel, package) subscription_sync_total{name=~"addon-operator|aws-vpce-operator|custom-domains-operator|managed-node-metadata-operator|managed-upgrade-operator|must-gather-operator|ocm-agent-operator|osd-metrics-exporter|rbac-permissions-operator|openshift-splunk-forwarder-operator|aws-account-operator|certman-operator|cloud-ingress-operator|configure-alertmanager-operator|deadmanssnitch-operator|deployment-validation-operator|dynatrace-operator|gcp-project-operator|managed-velero-operator|observability-operator|opentelemetry-operator|pagerduty-operator"} )
          record: sre:operators:succeeded
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-pending-csr-alert
spec:
  groups:
    - name: sre-pending-csr-alert
      rules:
        - alert: CSRPendingLongDurationSRE
          annotations:
            message: MAPI CSR requests have been pending for more then 15 minutes. This can indicate that orphaned VMs are being created and requires immeditate remediation.
          expr: sum(mapi_current_pending_csr) > 1
          for: 15m
          labels:
            namespace: openshift-monitoring
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-proxy-managed-notification-alerts
spec:
  groups:
    - name: sre-proxy-managed-notification-alerts
      rules:
        - alert: AdditionalTrustBundleCAExpiringNotificationSRE
          annotations:
            message: Additional Trust Bundle CA certificate will expire in {{ $value | humanizeDuration }}. Ensure new certificate is provided prior to expiration to avoid cluster degradation and/or unavailability
          expr: cluster_proxy_ca_expiry_timestamp{name="osd_exporter"} - time() < 86400 * 30 > 0
          for: 10m
          labels:
            managed_notification_template: AdditionalTrustBundleCAExpiring
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: Info
        - alert: AdditionalTrustBundleCAExpiredNotificationSRE
          annotations:
            message: Additional Trust Bundle CA certificate will expire in {{ $value | humanizeDuration }}. Ensure new certificate is provided prior to expiration to avoid cluster degradation and/or unavailability
          expr: cluster_proxy_ca_expiry_timestamp{name="osd_exporter"} - time() <= 0
          for: 10m
          labels:
            managed_notification_template: AdditionalTrustBundleCAExpired
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: Info
        - alert: AdditionalTrustBundleCAInvalidNotificationSRE
          annotations:
            message: Cluster proxy CA has failed validation. Ensure the CA is PEM-encoded X.509.
          expr: cluster_operator_conditions{job="cluster-version-operator", name="network", condition="Degraded",reason="TrustBundleValidationFailure"} == 1
          for: 5m
          labels:
            managed_notification_template: AdditionalTrustBundleCAInvalid
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: Info
        - alert: ClusterProxyNetworkDegradedNotificationSRE
          annotations:
            message: Cluster proxy is failing network readiness endpoint checks and may be misconfigured or not running.
          expr: cluster_operator_conditions{job="cluster-version-operator", name="network", condition="Degraded",reason="InvalidProxyConfig"} == 1
          for: 10m
          labels:
            managed_notification_template: ClusterProxyNetworkDegraded
            namespace: openshift-monitoring
            send_managed_notification: "true"
            severity: Info
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-pruning
spec:
  groups:
    - name: sre-pruning
      rules:
        - alert: PruningCronjobErrorSRE
          annotations:
            message: SRE Pruning Job {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than thirty minutes to complete.
          expr: kube_cronjob_status_active{namespace="openshift-sre-pruning"}>0
          for: 30m
          labels:
            namespace: openshift-sre-pruning
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-pv
spec:
  groups:
    - name: sre-pv-customer
      rules:
        - alert: KubePersistentVolumeUsageCriticalCustomer
          annotations:
            message: The customer PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value }}% free.
          expr: |
            100
            * kubelet_volume_stats_available_bytes{job="kubelet",namespace!~"(^openshift-.*|^kube-.*|^default$|^redhat-.*)"}
            / kubelet_volume_stats_capacity_bytes{job="kubelet",namespace!~"(^openshift-.*|^kube-.*|^default$|^redhat-.*)"}
            < 3
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: critical
        - alert: KubePersistentVolumeFullInFourDaysCustomer
          annotations:
            message: Based on recent sampling, the customer PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ printf "%0.2f" $value }}% is available.
          expr: |
            100
            * (kubelet_volume_stats_available_bytes{job="kubelet",namespace!~"(^openshift-.*|^kube-.*|^default$|^redhat-.*)"}
            / kubelet_volume_stats_capacity_bytes{job="kubelet",namespace!~"(^openshift-.*|^kube-.*|^default$|^redhat-.*)"})
            < 15 and predict_linear(kubelet_volume_stats_available_bytes{job="kubelet",namespace!~"(^openshift-.*|^kube-.*|^default$|^redhat-.*)"}[6h],
            4 * 24 * 3600) < 0
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
    - name: sre-pv-lp
      rules:
        - alert: KubePersistentVolumeUsageCriticalLayeredProduct
          annotations:
            message: The customer PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value }}% free.
          expr: |
            100
            * kubelet_volume_stats_available_bytes{job="kubelet",namespace=~"(^redhat-.*)",namespace!~"(^redhat-rhmi.*)"}
            / kubelet_volume_stats_capacity_bytes{job="kubelet",namespace=~"(^redhat-.*)",namespace!~"(^redhat-rhmi.*)"}
            < 3
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: critical
        - alert: KubePersistentVolumeFullInFourDaysLayeredProduct
          annotations:
            message: Based on recent sampling, the customer PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ printf "%0.2f" $value }}% is available.
          expr: |
            100
            * (kubelet_volume_stats_available_bytes{job="kubelet",namespace=~"(^redhat-.*)",namespace!~"(^redhat-rhmi.*)"}
            / kubelet_volume_stats_capacity_bytes{job="kubelet",namespace=~"(^redhat-.*)",namespace!~"(^redhat-rhmi.*)"})
            < 15 and predict_linear(kubelet_volume_stats_available_bytes{job="kubelet",namespace=~"(^redhat-.*)",namespace!~"(^redhat-rhmi.*)"}[6h],
            4 * 24 * 3600) < 0
          labels:
            namespace: '{{ $labels.namespace }}'
            severity: warning
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-router-health
spec:
  groups:
    - name: sre-router-health
      rules:
        - alert: RouterAvailabilityLT50PctSRE
          expr: "(sum(avg_over_time(kube_replicationcontroller_status_ready_replicas[1h])) by (replicationcontroller,namespace) \n/\non(replicationcontroller,namespace)\nmax(kube_replicationcontroller_status_replicas{namespace=\"default\",replicationcontroller=~\"router.?-.*\"} >0) by (replicationcontroller,namespace)) < 0.5\n"
          labels:
            namespace: openshift-monitoring
            severity: warning
        - alert: RouterAvailabilityLT30PctSRE
          expr: "(sum(avg_over_time(kube_replicationcontroller_status_ready_replicas[1h])) by (replicationcontroller,namespace) \n/\non(replicationcontroller,namespace)\nmax(kube_replicationcontroller_status_replicas{namespace=\"default\",replicationcontroller=~\"router.?-.*\"} >0) by (replicationcontroller,namespace)) < 0.3\n"
          labels:
            namespace: openshift-monitoring
            severity: critical
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: user-workload
    role: alert-rules
  name: user-workload-sre-runaway-sdn-preventing-container-creation
spec:
  groups:
    - name: sre-runaway-sdn-preventing-container-creation
      rules:
        - alert: RunawaySDNPreventingContainerCreationSRE
          annotations:
            description: SDN is consuming excessive memory rendering {{ $labels.node }} unusable. The SDN pod on this node may need to be cycled or the node may need to be replaced until OCPBUGS-773 or RHOCPPRIO-33 is addressed.
          expr: sum(sum(kube_pod_container_status_waiting_reason{reason="ContainerCreating"}) by (pod,namespace) * on (pod,namespace) group_right kube_pod_info) by (node) > 0 and sum(container_memory_usage_bytes{namespace="openshift-sdn"} > 8000000000) by (node)
          for: 10m
          labels:
            namespace: openshift-monitoring
            severity: critical
